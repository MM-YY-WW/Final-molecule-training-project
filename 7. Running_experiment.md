### 7. Running Experiment
This week we will learn more about the useful functions and packages that help run experiments and capture the experiment results. 

## 7.1 ArgParser

[ArgParser Documentation](https://docs.python.org/3/library/argparse.html)

This package is commonly used in programming projects. This could add the argument to code in the command line without modifying the already finished code. 

Example:
```
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--example1', type=int, default=0, help='you could give an integer with the python run.py --example1 9')
parser.add_argument('--example2', type=str, default='nothing', help='you could give a string with the python run.py --example2 'some input'')

args = parser.parse_args()
example1_value = args.example1 # 9
example2_value = args.example2 # 'some input'

# If do not pass any value just run python run.py
example1_default = args.example1 # 0
example2_default = args.example2 # 'nothing'

```

## 7.2 Logger

[Logger Documentation](https://www.geeksforgeeks.org/logging-in-python/)

The outputs of experiments are long. If we print them into the command line we need to record the results manually. When we conduct multiple experiments together, it is hard to follow and easy to make mistakes. This Logger could help record the output to a specific file. 

Example:
```
import logging

logging.basicConfig(filename="/projects/experiment1.log", format='%(asctime)s %(message)s', filemode='w')
logger = logging.getLogger()

logger.setLevel(logging.DEBUG)

logging.info("this sentence will be write to /project/experiment1.log")

```

## 7.3 f-string

[F string in Python](https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/)

This is usually used in the print() function

Example 
```
loss = 0.8
accuracy = 0.9

# Print with out f-string

print('loss: ', loss)
print('accuracy: ', accuracy)

# Print with f-string
print(f'The Loss is: {loss}, Accuracy is: {Accuracy}')

# Combine with the logger
logging.info(f'Loss: {loss}, Accuracy: {Accuracy}')
```

## 7.4 Shell Script

We could also write Python scripts to run multiple experiments at the same time. The script file is end by .sh
This is a place to write the code that we want to write into the command line, such as:
```
python run.py --experiment 1
python run.py --experiment 2
python run.py --experiment 3
python run.py --experiment 4
...
```
Otherwise, we need to open multiple terminals to run these experiments.

Example:

1. Create a folder called scripts/ to put all of the .sh files into it. 

2. Create a .sh file with the name "run_bbbp.sh"

We run 3 seeds for each dataset and each pretrain model. Here we use bbbp dataset and the masking pre-train model as an example

First, check the arguments that we could pass to the python file 

In chem/finetune.py: 
```
    parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')
    parser.add_argument('--device', type=int, default=0,
                        help='which gpu to use if any (default: 0)')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='input batch size for training (default: 32)')
    parser.add_argument('--epochs', type=int, default=2,
                        help='number of epochs to train (default: 100)')
    parser.add_argument('--lr', type=float, default=0.0001,
                        help='learning rate (default: 0.001)')
    parser.add_argument('--lr_scale', type=float, default=1,
                        help='relative learning rate for the feature extraction layer (default: 1)')
    parser.add_argument('--decay', type=float, default=0,
                        help='weight decay (default: 0)')
    parser.add_argument('--num_layer', type=int, default=5,
                        help='number of GNN message passing layers (default: 5).')
    parser.add_argument('--emb_dim', type=int, default=300,
                        help='embedding dimensions (default: 300)')
    parser.add_argument('--dropout_ratio', type=float, default=0.5,
                        help='dropout ratio (default: 0.5)')
    parser.add_argument('--graph_pooling', type=str, default="mean",
                        help='graph level pooling (sum, mean, max, set2set, attention)')
    parser.add_argument('--JK', type=str, default="last",
                        help='how the node features across layers are combined. last, sum, max or concat')
    parser.add_argument('--gnn_type', type=str, default="gin")
    parser.add_argument('--dataset', type=str, default = 'bbbp', help='root directory of dataset. For now, only classification.')
    parser.add_argument('--input_model_file', type=str, default = 'chem/model_gin/masking.pth', help='filename to read the model (if there is any)')
    parser.add_argument('--filename', type=str, default = '', help='output filename')
    parser.add_argument('--seed', type=int, default=55, help = "Seed for splitting the dataset.")
    parser.add_argument('--runseed', type=int, default=0, help = "Seed for minibatch selection, random initialization.")
    parser.add_argument('--split', type = str, default="random_scaffold", help = "random or scaffold or random_scaffold")
    parser.add_argument('--eval_train', type=int, default = 0, help='evaluating training or not')
    parser.add_argument('--num_workers', type=int, default = 4, help='number of workers for dataset loading')
    args = parser.parse_args()

```
So we could define our hyperparameter according to this:
In ./scripts/run_bbbp.sh, write

```
#!/bin/bash
python finetune.py --device 0 --runseed 0 --batch_size 32 --epochs 100 --lr 0.0001 --dataset bbbp --input_model_file 'chem/model_gin/masking.pth' --split 'scaffold'

python finetune.py --device 0 --runseed 1 --batch_size 32 --epochs 100 --lr 0.0001 --dataset bbbp --input_model_file 'chem/model_gin/masking.pth' --split 'scaffold'

python finetune.py --device 0 --runseed 2 --batch_size 32 --epochs 100 --lr 0.0001 --dataset bbbp --input_model_file 'chem/model_gin/masking.pth' --split 'scaffold'
``` 
Before running the script we need to make sure we have enough resources to run, open nvtop
- the device is which GPU to use
- run seed is the different seed to run the experiment, we do 0, 1, 2 three different seeds
- When running code, two parts require MEM, the model itself and the data. Usually, the model will not be pretty large. To see the estimated size of the model you could pass --batch_size 1 to see how much memory it needs to load the pre-trained model. The meaning of batch is how many pieces of data are sent into the model at once. Then increase the batch_size you could estimate how much memory it takes to load one piece of data. Notice that we will run three experiments at once. Then estimate the batch_size is needed to avoid the "CUDA_OUT_OF_MEMORY" error.
- epoch is how many times we finetuned the code. In one epoch all of the training data will be learned once.
- lr is the learning rate, representing how much we modify the original pre-trained parameters. Too large will make the model hard to convergence. Too small will make the model convergence too slow.
- dataset is the name of dataset to run
- input_model_file is the path to the selected pre-trained model
- split is how to split the dataset into train, valid, and test sets.

3. Write the command of running code
```
python finetune.py 
```

4. In the command line put the following command to make this shell script have permission to execute
```
chmod u+x ./scripts/run_bbbp.sh
```

5. Put the following into command line to run the script

```
./scripts/run_bbbp.sh
```

6. How to kill the process created by shell script
Be careful and check multiple times to make sure you kill the correct process

1. Kill process by pid
In the command line:
```
kill <pid of the process>
```
3. use nvtop
- Open the nvtop and find the process of this script
- Click the process you want to kill and then click 'F9Kill' or press F9 button
- select 9 SIGKILL in the left part Send signal column
- Press Enter to kill the process






