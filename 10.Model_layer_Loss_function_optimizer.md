
# Introduction to PyTorch Layers, Loss Functions, and Optimizers

## 1. PyTorch Model Layers

PyTorch provides a wide variety of neural network layers that can be used to build models. Here are some of the most commonly used layers:

### 1.1 Linear Layer
[torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)

- The linear layer applies a linear transformation to the incoming data: y = xA<sup>T</sup> + b.

- [Matrix Definitions](https://online.stat.psu.edu/statprogram/reviews/matrix-algebra/definitions)

- [Matrix Multiplications](https://online.stat.psu.edu/statprogram/reviews/matrix-algebra/arithmetic)

  - The shape of A is (out_features, out_features), which is used to transform the dimension of x (in_festures) to the dimension of y (out_features).

  - For example, if the batch size is 4 and each sample is represented by 2 float numbers, the dimension of x should be (4, 2). 

  - $` x = [[3,4], [2,3], [3,3], [4,3]] `$

  - Therefore, the in_features should be the last dimension of input x, which is 2, the length of each sample.

  - Suppose we want to transform the length of the input sample from 2 to 1, then the out_features should equal 1.

  - The dimension of A<sup>T</sup> is (in_features, out_features) = (2,1).  The dimension of A is (out_features, out_features)

  - x



[Linear Algebra: Example 5.1.1](https://math.libretexts.org/Bookshelves/Linear_Algebra/A_First_Course_in_Linear_Algebra_(Kuttler)/05%3A_Linear_Transformations/5.01%3A_Linear_Transformations)

```python
import torch
import torch.nn as nn

linear_layer = nn.Linear(in_features=10, out_features=5)
x = torch.randn(2, 10)
output = linear_layer(x)
print(output)
```

### 1.2 Dropout Layer
The dropout layer is used to prevent overfitting by randomly setting a fraction of the input units to 0 at each update during training time.
```python
dropout_layer = nn.Dropout(p=0.5)
x = torch.randn(3, 5)
output = dropout_layer(x)
print(output)
```

## 2. PyTorch Loss Functions

Loss functions measure how well the model's output matches the target values. Some commonly used loss functions in PyTorch are:

### 2.1 Mean Squared Error Loss (MSELoss)
```python
criterion = nn.MSELoss()
predictions = torch.tensor([0.2, 0.5, 0.8], requires_grad=True)
targets = torch.tensor([0.3, 0.4, 0.9])
loss = criterion(predictions, targets)
print(loss)
```

### 2.2 Cross Entropy Loss
```python
criterion = nn.CrossEntropyLoss()
predictions = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)
targets = torch.tensor([0])
loss = criterion(predictions, targets)
print(loss)
```

### 2.3 Binary Cross-Entropy Loss
```python
criterion = nn.BCELoss()
predictions = torch.tensor([0.8, 0.6], requires_grad=True)
targets = torch.tensor([1.0, 0.0])
loss = criterion(predictions, targets)
print(loss)
```

## 3. PyTorch Optimizers

Optimizers adjust the weights of the model based on the gradients computed during backpropagation. Common optimizers include:

### 3.1 Stochastic Gradient Descent (SGD)
```python
optimizer = torch.optim.SGD(linear_layer.parameters(), lr=0.01)
optimizer.zero_grad()
output = linear_layer(x)
loss = criterion(output, torch.randn(2, 5))
loss.backward()
optimizer.step()
```

### 3.2 Adam Optimizer
```python
optimizer = torch.optim.Adam(linear_layer.parameters(), lr=0.001)
optimizer.zero_grad()
output = linear_layer(x)
loss = criterion(output, torch.randn(2, 5))
loss.backward()
optimizer.step()
```

## 4. Example: Simple Neural Network

Hereâ€™s a simple example combining layers, a loss function, and an optimizer:
```python
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 20)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(20, 1)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = SimpleNN()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Example forward and backward pass
x = torch.randn(5, 10)
target = torch.randn(5, 1)
output = model(x)
loss = criterion(output, target)
optimizer.zero_grad()
loss.backward()
optimizer.step()
print(f'Loss: {loss.item()}')
```

