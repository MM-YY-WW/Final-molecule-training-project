### 6. Pretrain GNN Finetune
This week we will practice an example of reading through the paper and reproduce the result reported in the paper by using the official implementation they provided in the paper. 

## 6.1 Paper and Slides:
Read the paper: "[Strategies for Pre-training Graph Neural Networks](https://arxiv.org/abs/1905.12265)" 

[Pre-train GNN Slides](https://docs.google.com/presentation/d/1v73ZH6Kp7lPMBR8DBEgSAd7Bsv3aoVvq57qgSleGjCg/edit?usp=sharing) are available

Answer the following questions:
1. What is the background of the authors? Which school which lab?
2. When and which conference is this paper published?
3. What is pre-train model?
4. Why do we need to finetune the pre-trained model?
5. What are the pre-train strategies the author proposed? What are the pre-train tasks?
6. What are the meanings of the train/valid/test sets?
7. Which datasets are they used to evaluate their model performance?
   
## 6.2 Code Implementation
Find the official implementation of the paper: "Strategies for Pre-training Graph Neural Networks" on GitHub. Clone the repository to the local machine.

### 6.2.1 Clone GitHub repository
[Git and Github](https://github.com/MM-YY-WW/Final-molecule-training-project/blob/main/1.Preliminary.md#14-git-and-github)

Tips: Search paper title + GitHub in google could find the official implementation of a paper. 

For example [GitHub pretrain-gnns](https://github.com/snap-stanford/pretrain-gnns)

### 6.2.2 Setup Conda Environment
[Set up Conda Environment](https://github.com/MM-YY-WW/Final-molecule-training-project/blob/main/1.Preliminary.md#11-set-up-conda-environment)

The detailed environment setup can be found in the README file of the official implementation of the paper. 

### 6.2.3 Tmux
[Tmux](https://tmuxcheatsheet.com/)

1. Tmux is a tool helping you keep the code running even the terminal is closed.

2. Start a new tmux session:
```
tmux new-session -t <your session name>
```

3. Allow mouse usage in tmux terminal
   - first enter command mode by typing Ctrl+B :
     then toggle the mouse on (or off) with the command set -g mouse
4. activate the conda environment again

5. to list out all the tmux sessions created previously
```
tmux list-sessions
```
6. to resume a tumx session created previously
```
tmux attach -t <tmux session id>
```

### 6.2.4 Run pretrain-gnn finetune code with chem datasets

1. The instructions for running the finetune experiment will also shown in the README file of the official implementation of the paper.

2. We could reproduce paper results according to the settings in the paper, such as "we evaluate our model by using 5 random seed and calculate the average".
   - However, different paper have different settings. When we want to compare the performance of the models from different papers, we need a uniform setting
   - Usually, we run ten experiments for seed 0-9 and calculate the average performance for each dataset.
     
3. For each chem dataset we used before in [Download and evaluate molecular datasets](https://github.com/MM-YY-WW/Final-molecule-training-project/blob/main/3.Data_Structure.md#32-download-and-investigate-molecular-datasets), evaluate and record the model performance with different strategies reported in the paper. Find the best strategies. 
